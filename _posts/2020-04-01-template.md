---
layout: post
title: Learning from geometric graphs, a geometric deep learning perspective
tags: []
authors: Wang, Zichen, Amazon Web Services; Shi, Yunzhi, Amazon Web Services; Chen, Xin, Amazon Web Services
---

This is a blog post about the ICLR2021 paper:

- Bowen Jing, Stephan Eismann, Patricia Suriana, Raphael J Townshend, Ron Dror (2021): [**Learning from Protein Structure with Geometric Vector Perceptrons.**](https://iclr.cc/virtual/2021/spotlight/3449)

## Graphs and geometric graph
Leaning on the success of deep neural networks on images, texts, and audios, graph neural networks (GNN) have been an active area of machine learning research to tackle various problems in graph data. Graph is a power way of representing relationships among entities as nodes connected by edges. Nodes and edges can have features on them for GNN to learn from. 

Sometimes nodes and edges can have spatial features, such as 3D coordinates of nodes and directions along edges, adding a layer of geometric relationships (including distance, direction, angle, etc.) among the entities. In this post, we term this type of graph with vector features on nodes and/or edges as *geometric graph*. Small molecules, macro-molecules, and geospatial data are all examples where one can construct geometric graphs: small molecules can be represented by a graph of atoms connected by chemical bonds, where atoms can be described by their coordinates in 3D Euclidean space and chemical bonds can be described by vectors along their directions. The angles between adjacent chemical bonds are known to be indicative of the chemical properties of the molecule such as stability. Similarly, macro-molecules such as proteins can be represented by a graph of amino acid residues connected based on their spatial proximity. Each amino acid residues contain $k$ key atoms, making the node vector feature to be $\mathbf{V}_n \in \mathbb{R}^{k \times 3}$. Spatial networks or transportation network can be constructed by treating intersections as nodes and roads as edges from urban geospatial data. 

## A Geometric (Deep Learning) perspective
The term “geometry” in a narrow sense is equivalent to the classic Euclidean geometry. However, other schools geometries exist such as Riemannian geometry. A unifying broader sense of geometry, Klein’s Erlangen Program, views it as a study of *invariance* with respect to various transformations.

In the broader sense geometry, graph is also an important geometric object because its nodes are invariant to permutations: ordering of its nodes do not change the property of the entire graph. On a local level, ordering of direct neighbors does not affect a node attributes such as degree and centrality. With this perspective, when we impose Euclidean geometric features onto a graph to form a *geometric graph*, it is further invariant to Euclidean group transformations $E(n)$ including rotations, translations and reflections. 

To develop an effective geometric deep learning algorithm on *geometric graphs*, one need to extend the GNN to operate in $E(3)$ symmetries (assuming we are dealing with 3D Euclidean space). 

## Introducing the GVP paper
Most GNN architectures are unable to reason over the spatial relationships among the nodes and edges in a geometric graph. In this post, we will discuss a paper published in ICLR 2021: 

* Bowen Jing, Stephan Eismann, Patricia Suriana, Raphael J Townshend, Ron Dror (2021): Learning from Protein Structure with Geometric Vector Perceptrons. https://iclr.cc/virtual/2021/spotlight/3449

The paper focuses on tasks operating on the tertiary structures of proteins and frames the method as “learning from structures”. The problem of learning from geometric graph can be naively solved by ignoring either aspect of the data: 1) relational reasoning on graphs using GNN while ignoring geometric, and 2) geometric reasoning on structure using 3D CNN with Voxelized representations. Motivated by combining the best from both worlds of GNN and CNN, the authors developed GVP-GNN, which is a novel GNN architecture with Geometric Vector Perceptrons (GVP) as building blocks. 

### GVP, the Euclid’s neuron 
As its name suggests, GVP is a type of perceptron, just like dense layer in MLP, but can jointly process data points with both scalar and vector features. 

How does GVP do that? Let’s first revisit how a perceptron work. We denote $\mathbf{s} \in \mathbb{R}^h$ as a feature vector where each value in $\mathbf{s}$ has nothing to do with spatial information. A perceptron is simply:

$$
\begin{equation}
    \mathbf{s}' = Perceptron(\mathbf{s}) = \sigma(\mathbf{w}^\intercal\mathbf{s} + b)
\end{equation}
$$

A GVP deals with a data point that possess vector features $\mathbf{V} \in \mathbb{R}^{\nu \times 3}$ and jointly process $\mathbf{s}$ and $\mathbf{V}$ with the following computations:
$$
\begin{equation}
    \mathbf{s}', \mathbf{V}' = GVP(\mathbf{s}, \mathbf{V})
\end{equation}
$$

$$
\begin{equation}
    \mathbf{s}' = \sigma(\mathbf{W}_m
    \begin{bmatrix}
        \| \mathbf{W}_h \mathbf{V} \|_2\\
        \mathbf{s}
    \end{bmatrix}  + \mathbf{b})
\end{equation}
$$

$$
\begin{equation}
    \mathbf{V}' = \sigma^+(\| \mathbf{W}_{\mu} \mathbf{W}_{h} \mathbf{V} \|_2) \odot \mathbf{W}_{\mu} \mathbf{W}_{h} \mathbf{V} 
\end{equation}
$$

The output $\mathbf{s}'$, $\mathbf{V}'$ from GVP is provable $E(3)$ invariant and equivariant, respectively, with respect to rotations and reflections. As we can easily see in equation (3) that the scalar output $\mathbf{s}'$ only depends on the l2-norm of the vector, which is constant after any rotations/reflections. Similarly, in equation (4), the directions in the vector output $\mathbf{V}'$ is a function of $\mathbf{W}_{\mu} \mathbf{W}_{h} \mathbf{V}$, which means it only changes with transformation of $\mathbf{V}$. 

The authors further extended the universal approximation theorem: GVP can approximate any rotation-invariant functions.


Now that we have GVP to take care of the Euclidean transformations, we can use it to build a GNN to tackle geometric graphs. 

### Refresher on GNN basics

GNN typically reasons among neighboring nodes by exchanging messages between them to update node representations with node and edge features, thus integrating the topological structures of the graph when making inferences. The message passing paradigm ([Gilmer et al. 2017](https://arxiv.org/abs/1704.01212)) unifies many GNN architectures as collecting and aggregating messages from neighbors to update the node representation. 


The message function describes how a pair of nodes and the edge connecting them produce the message: 
$$
\begin{equation}
    \mathbf{m}_{ij} = M(\mathbf{h}_i, \mathbf{h}_j, \mathbf{h}_{ij})    
\end{equation}
$$

Afterwards, messages collected from all neighbors of a node are aggregated and used for updating the node’s representation: 
$$
\begin{equation}
    \mathbf{h}_{i} \leftarrow U(\mathbf{h}_i, \sum_{j\in \mathcal{N}(i)}\mathbf{m}_{ij} )
\end{equation}
$$

### GVP-GNN: Message from vector features

GVP-GNN adopted the general message passing scheme of GNNs, allowing nodes and edges to have scalar and vector features. 

GVP-GNN message function:
$$
\begin{align}
\mathbf{m}_{ij} &= M_{GVP}(\mathbf{s}_j, \mathbf{V}_j, \mathbf{s}_{ij}, \mathbf{V}_{ij})\\
 &= GVP(concat(\mathbf{s}_j, \mathbf{s}_{ij}), concat(\mathbf{V}_j, \mathbf{V}_{ij})) \\
\end{align}
$$

We use $h=s, V$ to simplify notations, GVP-GNN update function:
$$
\begin{equation}
\mathbf{h}_{i} \leftarrow LayerNorm(\mathbf{h}_i + \frac{1}{|\mathcal{N}(i)|} Dropout( \sum_{j\in \mathcal{N}(i)}\mathbf{m}_{ij}))
\end{equation}
$$

GVP-GNN is not the first neural architecture to reason over geometric graphs. Other prominant algorithms developed prior to GVP-GNN include SchNet, Tensor Field Network (TFN), and SE(3)-Transformer. 

SchNet is a form of continous convolution on GNN
SchNet message function: continuous filter
$$
\begin{align}
\mathbf{m}_{ij} 
% &= M_{SchNet}(\mathbf{s}_j, \mathbf{V}_j, \mathbf{V}_{i}) \\
&=  \phi_{CF}(\Vert\mathbf{V}_j - \mathbf{V}_i\Vert) \phi_{S}(\mathbf{s}_j)
\end{align}
$$

TFN message function:
TFN uses spherical harmonics to compute its learnable weight kernel $\mathbf{W}^{lk} : \mathbb{R}^3 \rightarrow \mathbb{R}^{(2l+1)\times(2k+1)}$ to preserves $SE(3)$ equivariance. 
$$
\begin{align}
\mathbf{m}_{ij} 
% &= M_{TFN}(\mathbf{s}_j, \mathbf{V}_j, \mathbf{V}_{i}) \\
&= \sum_k\mathbf{W}^{lk}(\mathbf{V}_j - \mathbf{V}_i) \mathbf{s}_j^{lk}
\end{align}
$$

SE(3)-transformer message function:
SE(3)-transformer enhanced TFN with the attention mechanims. 

$$
\begin{align}
\mathbf{m}_{ij} 
% &= M_{SE3}(\mathbf{s}_i, \mathbf{V}_i, \mathbf{V}_{j}) \\
&= \sum_k\alpha_{ij} \mathbf{W}^{lk}(\mathbf{V}_j - \mathbf{V}_i) \mathbf{s}_j^{lk}
\end{align}
$$

Observing the message functions (equations 10, 11, 12) from the three preceeding algorithms, we note that they are depends on the direction between the vector features: $\mathbf{V}_j - \mathbf{V}_i$. GVP-GNN achieve $SE(3)$ equivariance of the message via concatentation of vector features of node **and** edge: $concat(\mathbf{V}_j, \mathbf{V}_{ij})$. The concatenation allows GVP-GNN to generate an ensemble of vector messages from both nodes and edges, making it more flexible/versatile than the competing methods. More concretely, the competing methods are unable to reason with edge's vector features that are indepedent of node's vector features. Having indepedent vector features from nodes and edges allows one to construct more expressive geometric graphs. 

For instance, in the protein graph, the authors encode unit vector in the direction of neighboring amnio acids as edge vector feature $C\alpha_j - C\alpha_i$, whereas node vector features can represent amino acid's interal direction along $C\beta_i - C\alpha_i$. 

(add image to illustrate protein graph’s node/edge vector features)

(add example of geospatial graph with independent node and edge vector features.)

### Inherently interpretable by visualizing the learned vector field

Because GVP-GNN learns and updates vector features at from nodes and edges, it’s possible to visualize the learned vector features at the intermediate GVP-GNN layers. The set of learned vector features on a geometric graph actually resembles a vector field. We can try to interprete the the learned vector field using intuitions from physics. For instance, we can imagine the learned vector field represents force or velocity, although such interpretation needs to be rigousously investigated. 

The authors visualized the learned node vector features on protein geometric graphs. You can clearly see distinctive patterns on different protein graphs: contracting (A and D), rotating (B) expanding (C). It would be really interesting to study if those learned vector field actually are correlated with atomic forces among the amino acid residues in the protein graph. 

![Figure 2 in the GVP paper]()

In additional to looking at the resultant vector fields (feature maps in CNN), one should also analyze GVP’s kernels ($\mathbf{W}_{\mu}$ and $\mathbf{W}_{h}$) because they are analogous to CNN’s kernels (aka filters), where lower level kernels learns to detect edge and texture and higher level learns more complex images components such as animal eyes (think DeepDream/[Inceptionism](https://ai.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html)). 

### Applications of GVP-GNN within and beyond macro-molecules

Jing et al. demonstrated the application of their GVP-GNN to various supervised learning tasks on proteins where they are represented as geometric graphs of amino acid residue. A natural next step is to model the protein geometric graphs at atomic level. With its expressiveness in geometric graph, it would be interesting to see if high-resolution atomic graph representation of proteins improve various benchmarks in protein biology including protein design, and model quality assessment. Looking ahead, GVP-GNN has potential in improving protein structure prediction. For instance, in the AlphaFold2 framework, invariant point attention (IPA) module is currently used to refine the high-resolution residue protein structure. Can GVP-GNN perform better than the IPA module?  

Geometric graphs constructed from other macro-molecules like RNA and small molecules are immediate testbed for GVP-GNN, we also foresee broader applications in other domains. 

(add geospatial data use cases)
* ETA prediction from GoogleMap+DeepMind: https://deepmind.com/blog/article/traffic-prediction-with-advanced-graph-neural-networks
* Edge vector features: 
    * road direction
* MLSL use cases: (to be anonymized if mention in this blog post)
    * HERE technology road accident prediction
    * BMW?
    * TomTom Transport mode prediction

* ~~Modeling N-body systems from physics?~~
* ~~*V* beyond 3D space?~~

* Application in graph drawing 

Graph drawing algorithms (aka network layout algorithms) are used to compute the 2D or 3D coordinates of nodes from a graph according to their local and global topological structures. They are commonly used for visualizing graphs and are closely related to some non-linear dimensionality reduction algorithms such as Laplacian Eigenmaps, which applies principal component analysis on the Lapacian matrix of the affinity matrix of the input data. Force-directed graph drawing algorithms, such as Fruchterman-Reingold algorithm, are iterative methods that typically defines attractive and repulsive forces among nodes based on whether they are connected, with the objective of minimizing the energy of the system until equilibrium. They are computationally expensive: cubic time to the number of nodes $\mathcal{O}(n^3)$. 

We revisit the graph drawing problem, it can be formulated as a node-level tasks on geometric graph: given a graph, we aim to computing the coordinates of nodes ($\mathbf{V}_i \in \mathbb{R}^{1 \times d}$, where $d$ equals 2 or 3, for visualizing in 2D or 3D) that minimize the energy of the system. GVP-GNN can be plugged in to as a function approximator to solve this problem, where the node's vector features can be initialized randomly, just like in force-directed graph drawing algorithms where the positions of the nodes are initialized randomly. 



## References (incomplete)
- On Spectral Clustering: Analysis and an algorithm, 2001 Andrew Y. Ng, Michael I. Jordan, Yair Weiss http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.19.8100
- Fruchterman, Thomas M. J.; Reingold, Edward M. (1991), "Graph Drawing by Force-Directed Placement", Software – Practice & Experience, Wiley, 21 (11): 1129–1164, doi:10.1002/spe.4380211102
